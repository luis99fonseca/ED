{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMPLATE \n",
    "\n",
    "  Training, and  Testing.  Measures of perfomance.\n",
    "  Evaluation of a classification pipeline: standard scaler and perceptron.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric: pandas and numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# graphics\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.gridspec as gridspec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Data\n",
    "\n",
    "Starting with toy data to understand the main characteristics of the models. \n",
    "1. Creating artificial data with two characteristics \n",
    "    might be useful.\n",
    "\n",
    "Scikit- learn provides also data sets with real data and all of them are avalaible and easily download and written in the proper formats \n",
    "\n",
    "https://scikit-learn.org/stable/datasets/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs, make_circles, make_moons, make_circles\n",
    "\n",
    "X_blobs, y_blobs = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=None)\n",
    "\n",
    "X_moon,y_moon= make_moons(n_samples=1000, shuffle=True, noise=None, random_state=None)\n",
    "\n",
    "X_cir, y_cir= make_circles(n_samples=1000, shuffle=True, noise=None, random_state=None, factor=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_blobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "\n",
    "ax1.scatter(X_blobs[y_blobs==0,0],X_blobs[y_blobs==0,1],marker='s',color='r',label='0' )\n",
    "ax1.scatter(X_blobs[y_blobs==1,0],X_blobs[y_blobs==1,1],marker='s',color='g',label='0' )\n",
    "#ax1.set_aspect('equal')\n",
    "ax2.scatter(X_moon[y_moon==0,0],X_moon[y_moon==0,1],marker='s',color='r',label='0' )\n",
    "ax2.scatter(X_moon[y_moon==1,0],X_moon[y_moon==1,1],marker='s',color='g',label='0' )\n",
    "\n",
    "ax3.scatter(X_cir[y_cir==0,0],X_cir[y_cir==0,1],marker='s',color='r',label='0' )\n",
    "ax3.scatter(X_cir[y_cir==1,0],X_cir[y_cir==1,1],marker='s',color='g',label='0' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model  Evaluation\n",
    "Read the section 4.5 (4.5.1 and 4.5.3)\n",
    "\n",
    "Tan , Steinback and Kumar: Introduction to Data Mining\n",
    "\n",
    "**or** the following  technical report\n",
    "\n",
    "https://arxiv.org/abs/1811.12808\n",
    "\n",
    "\n",
    "The following code introduces the main steps of the most used strategies to evaluate a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning  and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Data set\n",
    "X=X_moon\n",
    "y=y_moon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy of Evaluation I:  Train and Test with hold-out\n",
    "\n",
    "Aplicable if the data set is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=0)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()   \n",
    "Xs=scaler.fit_transform(X_train)\n",
    "\n",
    "Xtest=scaler.transform(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "#Create perceptron and assign hyperparameters (max_iter, eta0- learning rate)\n",
    "ppn=Perceptron(penalty=None, alpha=0.0001, fit_intercept=True, max_iter=20, tol=None, \n",
    "               eta0=0.1, n_jobs=1, random_state=0, class_weight=None, warm_start=False)\n",
    "\n",
    "#Learning\n",
    "ppn.fit(X_train,y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=ppn.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(ppn, X_test, y_test)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "1. Try to understand the contents of the confusion matrix . Describe each of the entries by your own words\n",
    "2. Using the confusion matrix estimate: accuracy, recall, precision  \n",
    "3. Use the other facilities of the package sklearn.metrics to estimate the scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy of Evaluation II:  K-fold train and test\n",
    "\n",
    "\n",
    "The most widely used strategy to evalaute a classifier when the data set is not long enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.model_selection import StratifiedKFold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppn=Perceptron(penalty=None, alpha=0.0001, fit_intercept=True, max_iter=20, tol=None, \n",
    "               eta0=0.1, n_jobs=1, random_state=0, class_weight=None, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pre-processing included in the classification pipeline\n",
    "##### 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = StratifiedKFold(n_splits=5)\n",
    "for train_indices, test_indices in k_fold.split(X,y):\n",
    "    scaler = StandardScaler() \n",
    "    X_train=X[train_indices]\n",
    "   \n",
    "    Xs=scaler.fit_transform(X_train)\n",
    "\n",
    "    Xtest=scaler.transform(X[test_indices])\n",
    "    \n",
    "    ppn.fit(Xs,y[train_indices])\n",
    "    \n",
    "    y_pred=ppn.predict(Xtest)\n",
    "    print(confusion_matrix(y[test_indices],y_pred))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: introduce in the loop  the calculation of scores using the facilities of the package sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
